{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11216338,"sourceType":"datasetVersion","datasetId":7004225},{"sourceId":11216349,"sourceType":"datasetVersion","datasetId":7004232},{"sourceId":309764,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":262870,"modelId":283991}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip uninstall torchtext -y\n!pip uninstall torch -y\n!pip install torch==2.2.0 torchtext==0.17.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T13:32:49.190054Z","iopub.execute_input":"2025-03-30T13:32:49.190354Z","iopub.status.idle":"2025-03-30T13:35:55.212100Z","shell.execute_reply.started":"2025-03-30T13:32:49.190312Z","shell.execute_reply":"2025-03-30T13:35:55.211013Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Skipping torchtext as it is not installed.\u001b[0m\u001b[33m\n\u001b[0mFound existing installation: torch 2.5.1+cu121\nUninstalling torch-2.5.1+cu121:\n  Successfully uninstalled torch-2.5.1+cu121\nCollecting torch==2.2.0\n  Downloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\nCollecting torchtext==0.17.0\n  Downloading torchtext-0.17.0-cp310-cp310-manylinux1_x86_64.whl.metadata (7.6 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (2024.12.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.0)\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.0)\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.0)\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.0)\n  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.0)\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.0)\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.0)\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.0)\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.0)\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.0)\n  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.0)\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==2.2.0 (from torch==2.2.0)\n  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.17.0) (4.67.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.17.0) (2.32.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.17.0) (1.26.4)\nCollecting torchdata==0.7.1 (from torchtext==0.17.0)\n  Downloading torchdata-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0) (12.6.85)\nRequirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.1->torchtext==0.17.0) (2.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.0) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchtext==0.17.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchtext==0.17.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchtext==0.17.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchtext==0.17.0) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchtext==0.17.0) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchtext==0.17.0) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.17.0) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.17.0) (3.10)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.17.0) (2025.1.31)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.0) (1.3.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchtext==0.17.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchtext==0.17.0) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchtext==0.17.0) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchtext==0.17.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchtext==0.17.0) (2024.2.0)\nDownloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torchtext-0.17.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m719.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading torchdata-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchdata, torchtext\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.23.4\n    Uninstalling nvidia-nccl-cu12-2.23.4:\n      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.7.77\n    Uninstalling nvidia-curand-cu12-10.3.7.77:\n      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.10.0 requires pylibraft-cu12==24.10.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.10.0 requires rmm-cu12==24.10.*, but you have rmm-cu12 25.2.0 which is incompatible.\ntorchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.2.0 which is incompatible.\ntorchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvtx-cu12-12.1.105 torch-2.2.0 torchdata-0.7.1 torchtext-0.17.0 triton-2.2.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import re\nimport string\nimport zipfile\n\nimport emoji\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.autograd as autograd\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom nltk.corpus import stopwords\nimport spacy\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import (\n    accuracy_score,\n    classification_report,\n    confusion_matrix,\n    roc_auc_score,\n)\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-30T13:36:36.512092Z","iopub.execute_input":"2025-03-30T13:36:36.512403Z","iopub.status.idle":"2025-03-30T13:36:45.209190Z","shell.execute_reply.started":"2025-03-30T13:36:36.512377Z","shell.execute_reply":"2025-03-30T13:36:45.208378Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/rahul2nd/dataset_instagram-scraper_2025-03-30_09-30-50-381.csv\")\ntest_df.rename(columns={'text': 'comment_text'}, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T13:38:29.894756Z","iopub.execute_input":"2025-03-30T13:38:29.895126Z","iopub.status.idle":"2025-03-30T13:38:29.931165Z","shell.execute_reply.started":"2025-03-30T13:38:29.895096Z","shell.execute_reply":"2025-03-30T13:38:29.930047Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"vocab = torch.load(\"/kaggle/input/vocab-file/vocab.pth\")\nPAD_IDX = vocab['<pad>']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T13:51:33.462499Z","iopub.execute_input":"2025-03-30T13:51:33.462940Z","iopub.status.idle":"2025-03-30T13:51:33.734114Z","shell.execute_reply.started":"2025-03-30T13:51:33.462903Z","shell.execute_reply":"2025-03-30T13:51:33.732848Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class Improved_BI_LSTM_GloVe(nn.Module):\n    def __init__(self, vocab_size, embed_dim, hidden_dim, pad_idx, output_dim):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n        \n        # Enhanced Architecture\n        self.lstm = nn.LSTM(embed_dim, hidden_dim, \n                           num_layers=2,              # Stacked LSTMs\n                           bidirectional=True, \n                           batch_first=True,\n                           dropout=0.3)               # Inter-layer dropout\n        \n        self.attention = nn.Linear(hidden_dim * 2, 1) # Simple attention mechanism\n        self.bn1 = nn.BatchNorm1d(hidden_dim * 2)     # Batch normalization\n        \n        self.fc = nn.Sequential(\n            nn.Linear(hidden_dim * 2, hidden_dim),\n            nn.ReLU(),\n            nn.BatchNorm1d(hidden_dim),\n            nn.Dropout(0.5),                          # Increased dropout\n            nn.Linear(hidden_dim, output_dim)\n        )\n        \n        # Initialize with kaiming normal for better convergence\n        for layer in [self.attention, *self.fc]:\n            if isinstance(layer, nn.Linear):\n                nn.init.kaiming_normal_(layer.weight)\n\n    def forward(self, text, lengths):\n        # Embedding with dropout\n        embedded = F.dropout(self.embedding(text), p=0.2, training=self.training)\n        \n        # Packed sequence\n        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n            embedded, lengths.cpu(), batch_first=True, enforce_sorted=False\n        )\n        \n        # BiLSTM with 2 layers\n        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n        \n        # Attention mechanism\n        attention_weights = F.softmax(self.attention(output), dim=1)\n        context_vector = torch.sum(attention_weights * output, dim=1)\n        \n        # Batch norm + FC\n        context_vector = self.bn1(context_vector)\n        return self.fc(context_vector)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T13:51:35.915655Z","iopub.execute_input":"2025-03-30T13:51:35.916025Z","iopub.status.idle":"2025-03-30T13:51:35.924806Z","shell.execute_reply.started":"2025-03-30T13:51:35.915992Z","shell.execute_reply":"2025-03-30T13:51:35.923595Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport re\nimport string\nimport emoji\nfrom nltk.corpus import stopwords\nimport spacy\nfrom torchtext.data.utils import get_tokenizer\nfrom torch.nn.utils.rnn import pad_sequence\nimport numpy as np\n\nclass ToxicityClassifierPipeline:\n    def __init__(self, model, vocab_path=\"/kaggle/input/vocab-file/vocab.pth\"):\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model = model.to(self.device)\n        self.model.eval()\n        self.tokenizer = get_tokenizer(\"basic_english\")\n        self.vocab = torch.load(vocab_path)\n        self.PAD_IDX = self.vocab['<pad>']\n        self.nlp = spacy.load(\"en_core_web_sm\")\n        self.stpwds = stopwords.words('english')\n        \n        # Define preprocessing patterns\n        self.punc = string.punctuation.replace('#', '').replace('!', '').replace('?', '') + \"∞θ÷α•à−β∅³π‘₹´°£€\\\\×™√²—\"\n        self.patterns = [\n            r'\\\\[nrtbfv\\\\]',         # \\n, \\t etc\n            '<.*?>',                 # HTML tags\n            r'https?://\\S+|www\\.\\S+', # Links\n            r'\\ufeff',               # BOM characters\n            r'^[^a-zA-Z0-9]+$',      # Non-alphanumeric tokens\n            r'ｗｗｗ．\\S+',            # Full-width URLs\n            r'[\\uf700-\\uf7ff]',      # Unicode private-use chars\n            r'^[－—…]+$',            # Special punctuation\n            r'[︵︶]'                # CJK parentheses\n        ]\n        \n        # Chat words mapping (truncated for brevity)\n        self.chat_words = {\n            \"AFAIK\": \"As Far As I Know\",\n            \"AFK\": \"Away From Keyboard\",\n            # ... include all your chat words mapping\n        }\n        \n        self.time_zone_abbreviations = [\n            \"UTC\", \"GMT\", \"EST\", \"CST\", \"PST\", \"MST\",\n            \"EDT\", \"CDT\", \"PDT\", \"MDT\", \"CET\", \"EET\",\n            \"WET\", \"AEST\", \"ACST\", \"AWST\", \"HST\",\n            \"AKST\", \"IST\", \"JST\", \"KST\", \"NZST\"\n        ]\n\n    def preprocess_text(self, text):\n        \"\"\"Apply all preprocessing steps to a single text\"\"\"\n        if not isinstance(text, str) or not text.strip():\n            return \"\"\n            \n        # Apply regex patterns\n        for regex in self.patterns:\n            text = re.sub(regex, '', text)\n            \n        # Remove punctuation\n        text = text.translate(str.maketrans(self.punc, ' ' * len(self.punc)))\n        \n        # Remove time zones and stopwords\n        text = ' '.join(word for word in text.split() \n                       if word not in self.time_zone_abbreviations \n                       and word not in self.stpwds)\n        \n        # Expand chat words\n        text = ' '.join(self.chat_words.get(word.lower(), word) for word in text.split())\n        \n        # Lowercase and emoji handling\n        text = text.lower()\n        text = emoji.demojize(text)\n        text = re.sub(r'\\s+', ' ', text).strip()\n        \n        return text\n\n    def tokenize_and_numericalize(self, text, max_length=256):\n        \"\"\"Tokenize and convert to numerical tokens\"\"\"\n        if not text:  # Handle empty text\n            return torch.empty(0, dtype=torch.long)\n            \n        tokens = [token for token in self.tokenizer(text) if 1 < len(token) < 25]\n        tokens = tokens[:max_length]\n        numericalized = [self.vocab[token] if token in self.vocab else self.vocab['<unk>'] \n                        for token in tokens]\n        return torch.tensor(numericalized, dtype=torch.long)\n\n    def predict_toxicity(self, df, text_column='comment_text', batch_size=64):\n        \"\"\"\n        Predict toxicity for a DataFrame of texts\n        \n        Args:\n            df: Input DataFrame containing text to classify\n            text_column: Name of column containing text\n            batch_size: Batch size for prediction\n            \n        Returns:\n            DataFrame with original text and toxicity predictions\n        \"\"\"\n        # Create a copy of the original DataFrame to preserve indices\n        result_df = df.copy()\n        \n        # Preprocess all texts and keep track of non-empty texts\n        processed_data = []\n        valid_indices = []\n        \n        for idx, text in enumerate(df[text_column]):\n            processed = self.preprocess_text(text)\n            if processed:  # Only keep non-empty texts\n                processed_data.append(processed)\n                valid_indices.append(idx)\n        \n        # If all texts are empty after preprocessing\n        if not processed_data:\n            # Return all zeros for all predictions\n            result_df['toxic'] = 0\n            result_df['severe_toxic'] = 0\n            result_df['obscene'] = 0\n            result_df['threat'] = 0\n            result_df['insult'] = 0\n            result_df['identity_hate'] = 0\n            return result_df\n        \n        # Tokenize and numericalize only non-empty texts\n        tokenized = [self.tokenize_and_numericalize(text) for text in processed_data]\n        \n        # Create batches only for valid sequences\n        batches = []\n        for i in range(0, len(tokenized), batch_size):\n            batch_texts = tokenized[i:i+batch_size]\n            lengths = torch.tensor([len(t) for t in batch_texts])\n            \n            # Filter out empty sequences in this batch\n            valid_mask = lengths > 0\n            if not valid_mask.any():\n                continue\n                \n            batch_texts = [t for t, valid in zip(batch_texts, valid_mask) if valid]\n            lengths = lengths[valid_mask]\n            \n            # Pad sequences\n            padded = pad_sequence(batch_texts, batch_first=True, padding_value=self.PAD_IDX)\n            batches.append((padded, lengths, valid_mask))\n        \n        # Make predictions\n        all_preds = np.zeros((len(df), 6), dtype=int)  # Initialize with zeros\n        \n        with torch.no_grad():\n            current_idx = 0\n            for batch, lengths, valid_mask in batches:\n                batch = batch.to(self.device)\n                outputs = self.model(batch, lengths.to(self.device))\n                preds = (outputs > 0.5).int().cpu().numpy()\n                \n                # Assign predictions to the correct positions\n                batch_size = len(preds)\n                for i in range(batch_size):\n                    if current_idx + i < len(valid_indices):\n                        all_preds[valid_indices[current_idx + i]] = preds[i]\n                \n                current_idx += batch_size\n        \n        # Add predictions to result DataFrame\n        result_df['toxic'] = all_preds[:, 0]\n        result_df['severe_toxic'] = all_preds[:, 1]\n        result_df['obscene'] = all_preds[:, 2]\n        result_df['threat'] = all_preds[:, 3]\n        result_df['insult'] = all_preds[:, 4]\n        result_df['identity_hate'] = all_preds[:, 5]\n        \n        return result_df\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Load your trained model (example using the final model)\n    final_model = Improved_BI_LSTM_GloVe(\n        vocab_size=len(vocab),\n        embed_dim=100,\n        hidden_dim=256,\n        pad_idx=PAD_IDX,\n        output_dim=6\n    )\n    final_model.load_state_dict(torch.load(\"/kaggle/input/final/pytorch/default/1/final.pth\", map_location=torch.device('cpu')))\n\n    \n    # Initialize pipeline\n    pipeline = ToxicityClassifierPipeline(final_model)\n    \n    # Example test DataFrame\n    test_df = pd.read_csv(\"/kaggle/input/rahul2nd/dataset_instagram-scraper_2025-03-30_09-30-50-381.csv\")\n    test_df.rename(columns={'text': 'comment_text'}, inplace=True)\n    \n    # Get predictions\n    results = pipeline.predict_toxicity(test_df)\n    print(results[['comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T13:52:03.852780Z","iopub.execute_input":"2025-03-30T13:52:03.853169Z","iopub.status.idle":"2025-03-30T13:52:05.318877Z","shell.execute_reply.started":"2025-03-30T13:52:03.853137Z","shell.execute_reply":"2025-03-30T13:52:05.317692Z"}},"outputs":[{"name":"stdout","text":"              comment_text  toxic  severe_toxic  obscene  threat  insult  \\\n0    Why bro in sea always      0             0        0       0       0   \n1     😂😂😂😂😂😂😂 misericórdia      0             0        0       0       0   \n2                        🙌      0             0        0       0       0   \n3                        🔥      0             0        0       0       0   \n4               nigga what      1             1        1       0       1   \n5   Vessel of NBA youngboy      0             0        0       0       0   \n6                 JAJAJAJA      0             0        0       0       0   \n7                      wtf      1             0        1       0       1   \n8          fuck that bitch      1             1        1       0       1   \n9             Is this real      0             0        0       0       0   \n10           kill yourself      1             0        0       1       1   \n11         @hyunmin._.x 고고      0             0        0       0       0   \n12              @zellys_1w      0             0        0       0       0   \n13           @eluruchinche      0             0        0       0       0   \n14         La @ha__neth xd      0             0        0       0       0   \n\n    identity_hate  \n0               0  \n1               0  \n2               0  \n3               0  \n4               1  \n5               0  \n6               0  \n7               0  \n8               0  \n9               0  \n10              1  \n11              0  \n12              0  \n13              0  \n14              0  \n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"final_ans = results.drop('identity_hate',axis = 1)\nfinal_ans","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T13:53:47.671628Z","iopub.execute_input":"2025-03-30T13:53:47.671969Z","iopub.status.idle":"2025-03-30T13:53:47.694155Z","shell.execute_reply.started":"2025-03-30T13:53:47.671944Z","shell.execute_reply":"2025-03-30T13:53:47.692994Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"                   id            comment_text  toxic  severe_toxic  obscene  \\\n0   18069096703645733   Why bro in sea always      0             0        0   \n1   18052302512112156    😂😂😂😂😂😂😂 misericórdia      0             0        0   \n2   18119491555435268                       🙌      0             0        0   \n3   17884548831142635                       🔥      0             0        0   \n4   18059375443875265              nigga what      1             1        1   \n5   18097418647542771  Vessel of NBA youngboy      0             0        0   \n6   18071614537687683                JAJAJAJA      0             0        0   \n7   17947968803927907                     wtf      1             0        1   \n8   17916781080051191         fuck that bitch      1             1        1   \n9   18036427937403276            Is this real      0             0        0   \n10  17865902130262070           kill yourself      1             0        0   \n11  18045930944093198         @hyunmin._.x 고고      0             0        0   \n12  18050583443465416              @zellys_1w      0             0        0   \n13  17998000631608824           @eluruchinche      0             0        0   \n14  17915460810066302         La @ha__neth xd      0             0        0   \n\n    threat  insult  \n0        0       0  \n1        0       0  \n2        0       0  \n3        0       0  \n4        0       1  \n5        0       0  \n6        0       0  \n7        0       1  \n8        0       1  \n9        0       0  \n10       1       1  \n11       0       0  \n12       0       0  \n13       0       0  \n14       0       0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>18069096703645733</td>\n      <td>Why bro in sea always</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>18052302512112156</td>\n      <td>😂😂😂😂😂😂😂 misericórdia</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>18119491555435268</td>\n      <td>🙌</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>17884548831142635</td>\n      <td>🔥</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>18059375443875265</td>\n      <td>nigga what</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>18097418647542771</td>\n      <td>Vessel of NBA youngboy</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>18071614537687683</td>\n      <td>JAJAJAJA</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>17947968803927907</td>\n      <td>wtf</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>17916781080051191</td>\n      <td>fuck that bitch</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>18036427937403276</td>\n      <td>Is this real</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>17865902130262070</td>\n      <td>kill yourself</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>18045930944093198</td>\n      <td>@hyunmin._.x 고고</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>18050583443465416</td>\n      <td>@zellys_1w</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>17998000631608824</td>\n      <td>@eluruchinche</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>17915460810066302</td>\n      <td>La @ha__neth xd</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"final_ans.to_csv(\"result.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T13:54:41.076888Z","iopub.execute_input":"2025-03-30T13:54:41.077295Z","iopub.status.idle":"2025-03-30T13:54:41.093895Z","shell.execute_reply.started":"2025-03-30T13:54:41.077268Z","shell.execute_reply":"2025-03-30T13:54:41.092865Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
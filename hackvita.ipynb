{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8076,"databundleVersionId":44219,"sourceType":"competition"},{"sourceId":1483651,"sourceType":"datasetVersion","datasetId":870709},{"sourceId":11214804,"sourceType":"datasetVersion","datasetId":7003038}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Toxic Comment Classification with LSTM Models\n","metadata":{}},{"cell_type":"markdown","source":"## 1. Imports\n**Libraries used**\n\n- Core: PyTorch, Pandas, NumPy\n\n- NLP: TorchText\n\n- evaluation: Scikit-learn\n\n- Visualization: Matplotlib","metadata":{}},{"cell_type":"code","source":"!pip uninstall torchtext -y\n!pip uninstall torch -y\n!pip install torch==2.2.0 torchtext==0.17.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T11:52:05.588477Z","iopub.execute_input":"2025-03-30T11:52:05.588755Z","iopub.status.idle":"2025-03-30T11:54:34.558118Z","shell.execute_reply.started":"2025-03-30T11:52:05.588734Z","shell.execute_reply":"2025-03-30T11:54:34.557041Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Skipping torchtext as it is not installed.\u001b[0m\u001b[33m\n\u001b[0mFound existing installation: torch 2.5.1+cu121\nUninstalling torch-2.5.1+cu121:\n  Successfully uninstalled torch-2.5.1+cu121\nCollecting torch==2.2.0\n  Downloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\nCollecting torchtext==0.17.0\n  Downloading torchtext-0.17.0-cp310-cp310-manylinux1_x86_64.whl.metadata (7.6 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (2024.12.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.0)\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.0)\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.0)\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.0)\n  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.0)\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.0)\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.0)\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.0)\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.0)\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.0)\n  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.0)\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==2.2.0 (from torch==2.2.0)\n  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.17.0) (4.67.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.17.0) (2.32.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.17.0) (1.26.4)\nCollecting torchdata==0.7.1 (from torchtext==0.17.0)\n  Downloading torchdata-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0) (12.6.85)\nRequirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.1->torchtext==0.17.0) (2.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.0) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchtext==0.17.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchtext==0.17.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchtext==0.17.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchtext==0.17.0) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchtext==0.17.0) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchtext==0.17.0) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.17.0) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.17.0) (3.10)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.17.0) (2025.1.31)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.0) (1.3.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchtext==0.17.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchtext==0.17.0) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchtext==0.17.0) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchtext==0.17.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchtext==0.17.0) (2024.2.0)\nDownloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torchtext-0.17.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading torchdata-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchdata, torchtext\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.23.4\n    Uninstalling nvidia-nccl-cu12-2.23.4:\n      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.7.77\n    Uninstalling nvidia-curand-cu12-10.3.7.77:\n      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.10.0 requires pylibraft-cu12==24.10.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.10.0 requires rmm-cu12==24.10.*, but you have rmm-cu12 25.2.0 which is incompatible.\ntorchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.2.0 which is incompatible.\ntorchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvtx-cu12-12.1.105 torch-2.2.0 torchdata-0.7.1 torchtext-0.17.0 triton-2.2.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torchtext","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T11:58:24.918236Z","iopub.execute_input":"2025-03-30T11:58:24.918801Z","iopub.status.idle":"2025-03-30T11:58:24.922449Z","shell.execute_reply.started":"2025-03-30T11:58:24.918773Z","shell.execute_reply":"2025-03-30T11:58:24.921750Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import re\nimport string\nimport zipfile\n\nimport emoji\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.autograd as autograd\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom nltk.corpus import stopwords\nimport spacy\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import (\n    accuracy_score,\n    classification_report,\n    confusion_matrix,\n    roc_auc_score,\n)\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-30T11:58:13.880766Z","iopub.execute_input":"2025-03-30T11:58:13.881203Z","iopub.status.idle":"2025-03-30T11:58:21.386371Z","shell.execute_reply.started":"2025-03-30T11:58:13.881169Z","shell.execute_reply":"2025-03-30T11:58:21.385701Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## 2. Data Loading","metadata":{}},{"cell_type":"markdown","source":"**not important for final**","metadata":{}},{"cell_type":"code","source":"training_path = r\"/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip\"\nwith zipfile.ZipFile(training_path) as train_zip:\n    with train_zip.open(\"train.csv\") as csv:\n        training_data = pd.read_csv(csv)\n\ntraining_data.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T12:18:10.188256Z","iopub.execute_input":"2025-03-30T12:18:10.188583Z","iopub.status.idle":"2025-03-30T12:18:11.827927Z","shell.execute_reply.started":"2025-03-30T12:18:10.188558Z","shell.execute_reply":"2025-03-30T12:18:11.826764Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"                 id                                       comment_text  toxic  \\\n0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n\n   severe_toxic  obscene  threat  insult  identity_hate  \n0             0        0       0       0              0  \n1             0        0       0       0              0  \n2             0        0       0       0              0  \n3             0        0       0       0              0  \n4             0        0       0       0              0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000997932d777bf</td>\n      <td>Explanation\\nWhy the edits made under my usern...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000103f0d9cfb60f</td>\n      <td>D'aww! He matches this background colour I'm s...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000113f07ec002fd</td>\n      <td>Hey man, I'm really not trying to edit war. It...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0001b41b1c6bb37e</td>\n      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0001d958c54c6e35</td>\n      <td>You, sir, are my hero. Any chance you remember...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"sample_path = r\"/kaggle/input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip\"\nwith zipfile.ZipFile(sample_path) as z:\n    with z.open(\"sample_submission.csv\") as csv:\n        sample_data = pd.read_csv(csv)\n\nsample_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T12:18:11.894636Z","iopub.execute_input":"2025-03-30T12:18:11.895081Z","iopub.status.idle":"2025-03-30T12:18:12.107080Z","shell.execute_reply.started":"2025-03-30T12:18:11.895040Z","shell.execute_reply":"2025-03-30T12:18:12.106115Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"                 id  toxic  severe_toxic  obscene  threat  insult  \\\n0  00001cee341fdb12    0.5           0.5      0.5     0.5     0.5   \n1  0000247867823ef7    0.5           0.5      0.5     0.5     0.5   \n2  00013b17ad220c46    0.5           0.5      0.5     0.5     0.5   \n3  00017563c3f7919a    0.5           0.5      0.5     0.5     0.5   \n4  00017695ad8997eb    0.5           0.5      0.5     0.5     0.5   \n\n   identity_hate  \n0            0.5  \n1            0.5  \n2            0.5  \n3            0.5  \n4            0.5  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00001cee341fdb12</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0000247867823ef7</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00013b17ad220c46</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00017563c3f7919a</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00017695ad8997eb</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>0.5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"test_path = r\"/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip\"\nwith zipfile.ZipFile(test_path) as z:\n    with z.open(\"test.csv\") as csv:\n        test_data = pd.read_csv(csv)\n\ntest_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T12:18:12.492419Z","iopub.execute_input":"2025-03-30T12:18:12.492712Z","iopub.status.idle":"2025-03-30T12:18:13.817382Z","shell.execute_reply.started":"2025-03-30T12:18:12.492689Z","shell.execute_reply":"2025-03-30T12:18:13.816472Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"                 id                                       comment_text\n0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n3  00017563c3f7919a  :If you have a look back at the source, the in...\n4  00017695ad8997eb          I don't anonymously edit articles at all.","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00001cee341fdb12</td>\n      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0000247867823ef7</td>\n      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00013b17ad220c46</td>\n      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00017563c3f7919a</td>\n      <td>:If you have a look back at the source, the in...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00017695ad8997eb</td>\n      <td>I don't anonymously edit articles at all.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"test_labels_path = r\"/kaggle/input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip\"\nwith zipfile.ZipFile(test_labels_path) as z:\n    with z.open(\"test_labels.csv\") as csv:\n        test_labels_data = pd.read_csv(csv)\n\ntest_labels_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T12:18:14.726250Z","iopub.execute_input":"2025-03-30T12:18:14.726556Z","iopub.status.idle":"2025-03-30T12:18:14.901764Z","shell.execute_reply.started":"2025-03-30T12:18:14.726532Z","shell.execute_reply":"2025-03-30T12:18:14.901035Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"                 id  toxic  severe_toxic  obscene  threat  insult  \\\n0  00001cee341fdb12     -1            -1       -1      -1      -1   \n1  0000247867823ef7     -1            -1       -1      -1      -1   \n2  00013b17ad220c46     -1            -1       -1      -1      -1   \n3  00017563c3f7919a     -1            -1       -1      -1      -1   \n4  00017695ad8997eb     -1            -1       -1      -1      -1   \n\n   identity_hate  \n0             -1  \n1             -1  \n2             -1  \n3             -1  \n4             -1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00001cee341fdb12</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0000247867823ef7</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00013b17ad220c46</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00017563c3f7919a</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00017695ad8997eb</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":21},{"cell_type":"markdown","source":"## 3. Text Preprocessing","metadata":{}},{"cell_type":"code","source":"punc = string.punctuation\npunc.replace('#', '')\npunc.replace('!', '')\npunc.replace('?', '')\npunc = punc + \"∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—\"\n\nchat_words = {\n    \"AFAIK\": \"As Far As I Know\",\n    \"AFK\": \"Away From Keyboard\",\n    \"ASAP\": \"As Soon As Possible\",\n    \"ATK\": \"At The Keyboard\",\n    \"ATM\": \"At The Moment\",\n    \"A3\": \"Anytime, Anywhere, Anyplace\",\n    \"BAK\": \"Back At Keyboard\",\n    \"BBL\": \"Be Back Later\",\n    \"BBS\": \"Be Back Soon\",\n    \"BFN\": \"Bye For Now\",\n    \"B4N\": \"Bye For Now\",\n    \"BRB\": \"Be Right Back\",\n    \"BRT\": \"Be Right There\",\n    \"BTW\": \"By The Way\",\n    \"B4\": \"Before\",\n    \"B4N\": \"Bye For Now\",\n    \"CU\": \"See You\",\n    \"CUL8R\": \"See You Later\",\n    \"CYA\": \"See You\",\n    \"FAQ\": \"Frequently Asked Questions\",\n    \"FC\": \"Fingers Crossed\",\n    \"FWIW\": \"For What It's Worth\",\n    \"FYI\": \"For Your Information\",\n    \"GAL\": \"Get A Life\",\n    \"GG\": \"Good Game\",\n    \"GN\": \"Good Night\",\n    \"GMTA\": \"Great Minds Think Alike\",\n    \"GR8\": \"Great!\",\n    \"G9\": \"Genius\",\n    \"IC\": \"I See\",\n    \"ICQ\": \"I Seek you (also a chat program)\",\n    \"ILU\": \"ILU: I Love You\",\n    \"IMHO\": \"In My Honest/Humble Opinion\",\n    \"IMO\": \"In My Opinion\",\n    \"IOW\": \"In Other Words\",\n    \"IRL\": \"In Real Life\",\n    \"KISS\": \"Keep It Simple, Stupid\",\n    \"LDR\": \"Long Distance Relationship\",\n    \"LMAO\": \"Laugh My A.. Off\",\n    \"LOL\": \"Laughing Out Loud\",\n    \"LTNS\": \"Long Time No See\",\n    \"L8R\": \"Later\",\n    \"MTE\": \"My Thoughts Exactly\",\n    \"M8\": \"Mate\",\n    \"NRN\": \"No Reply Necessary\",\n    \"OIC\": \"Oh I See\",\n    \"PITA\": \"Pain In The A..\",\n    \"PRT\": \"Party\",\n    \"PRW\": \"Parents Are Watching\",\n    \"QPSA?\": \"Que Pasa?\",\n    \"ROFL\": \"Rolling On The Floor Laughing\",\n    \"ROFLOL\": \"Rolling On The Floor Laughing Out Loud\",\n    \"ROTFLMAO\": \"Rolling On The Floor Laughing My A.. Off\",\n    \"SK8\": \"Skate\",\n    \"STATS\": \"Your sex and age\",\n    \"ASL\": \"Age, Sex, Location\",\n    \"THX\": \"Thank You\",\n    \"TTFN\": \"Ta-Ta For Now!\",\n    \"TTYL\": \"Talk To You Later\",\n    \"U\": \"You\",\n    \"U2\": \"You Too\",\n    \"U4E\": \"Yours For Ever\",\n    \"WB\": \"Welcome Back\",\n    \"WTF\": \"What The F...\",\n    \"WTG\": \"Way To Go!\",\n    \"WUF\": \"Where Are You From?\",\n    \"W8\": \"Wait...\",\n    \"7K\": \"Sick:-D Laugher\",\n    \"TFW\": \"That feeling when\",\n    \"MFW\": \"My face when\",\n    \"MRW\": \"My reaction when\",\n    \"IFYP\": \"I feel your pain\",\n    \"TNTL\": \"Trying not to laugh\",\n    \"JK\": \"Just kidding\",\n    \"IDC\": \"I don't care\",\n    \"ILY\": \"I love you\",\n    \"IMU\": \"I miss you\",\n    \"ADIH\": \"Another day in hell\",\n    \"ZZZ\": \"Sleeping, bored, tired\",\n    \"WYWH\": \"Wish you were here\",\n    \"TIME\": \"Tears in my eyes\",\n    \"BAE\": \"Before anyone else\",\n    \"FIMH\": \"Forever in my heart\",\n    \"BSAAW\": \"Big smile and a wink\",\n    \"BWL\": \"Bursting with laughter\",\n    \"BFF\": \"Best friends forever\",\n    \"CSL\": \"Can't stop laughing\"\n}\n\n\nstpwds = stopwords.words('english')\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ntime_zone_abbreviations = [\n        \"UTC\", \"GMT\", \"EST\", \"CST\", \"PST\", \"MST\",\n        \"EDT\", \"CDT\", \"PDT\", \"MDT\", \"CET\", \"EET\",\n        \"WET\", \"AEST\", \"ACST\", \"AWST\", \"HST\",\n        \"AKST\", \"IST\", \"JST\", \"KST\", \"NZST\"\n    ]\n\npatterns = [\n    r'\\\\[nrtbfv\\\\]',         # \\n, \\t ..etc\n    '<.*?>',                 # Html tags\n    r'https?://\\S+|www\\.\\S+',# Links\n    r'\\ufeff',               # BOM characters\n    r'^[^a-zA-Z0-9]+$',      # Non-alphanumeric tokens\n    r'ｗｗｗ．\\S+',            # Full-width URLs\n    r'[\\uf700-\\uf7ff]',      # Unicode private-use chars\n    r'^[－—…]+$',            # Special punctuation-only tokens\n    r'[︵︶]'                # CJK parentheses\n]\n\ndef preprocess(text):\n    for regex in patterns:\n        text = re.sub(regex, '', text)\n    text = text.translate(str.maketrans(punc, ' ' * len(punc)))\n    text = ' '.join(word for word in text.split() if word not in time_zone_abbreviations)\n    text = ' '.join(word for word in text.split() if word not in stpwds)\n    text = ' '.join(chat_words.get(word.lower(), word) for word in text.split())\n    text = text.lower()\n    text = emoji.demojize(text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T12:18:15.672135Z","iopub.execute_input":"2025-03-30T12:18:15.672422Z","iopub.status.idle":"2025-03-30T12:18:16.280486Z","shell.execute_reply.started":"2025-03-30T12:18:15.672399Z","shell.execute_reply":"2025-03-30T12:18:16.279589Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n  warnings.warn(Warnings.W111)\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"## 4. Vocabulary Construction","metadata":{}},{"cell_type":"code","source":"comments = list(training_data[\"comment_text\"])\ntrain_iter = iter(comments)\n\ntokenizer = get_tokenizer(\"basic_english\")\n\ndef yield_tokens(data_iter):\n    for text in data_iter:\n        cleaned_text = preprocess(text)\n        tokens = [\n            token for token in tokenizer(cleaned_text)\n            if 1 < len(token) < 25\n        ]\n        yield tokens\n\n# Build vocabulary with size limit\nvocab = build_vocab_from_iterator(\n    yield_tokens(train_iter),\n    specials=[\"<pad>\", \"<unk>\"],\n    max_tokens=30002  # 30K + 2 special tokens for unkown tokens and padding\n)\nvocab.set_default_index(vocab[\"<unk>\"])\nPAD_IDX = vocab['<pad>']\n\nprint(f\"Final vocabulary size: {len(vocab)}\")\nprint(\"Sample valid tokens:\", [t for t in list(vocab.get_itos())[2:12]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T12:19:21.110125Z","iopub.execute_input":"2025-03-30T12:19:21.110420Z","iopub.status.idle":"2025-03-30T12:20:33.890649Z","shell.execute_reply.started":"2025-03-30T12:19:21.110390Z","shell.execute_reply":"2025-03-30T12:20:33.889891Z"}},"outputs":[{"name":"stdout","text":"Final vocabulary size: 30002\nSample valid tokens: ['article', 'the', 'page', 'wikipedia', 'talk', 'you', 'please', 'would', 'one', 'like']\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"torch.save(vocab, \"vocab.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T12:20:33.891562Z","iopub.execute_input":"2025-03-30T12:20:33.891847Z","iopub.status.idle":"2025-03-30T12:20:33.915865Z","shell.execute_reply.started":"2025-03-30T12:20:33.891824Z","shell.execute_reply":"2025-03-30T12:20:33.914989Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef text_pipeline(text):\n    return [\n        vocab[token] if token in vocab else vocab['<unk>'] \n        for token in tokenizer(text)\n    ]\n\ndef label_pipeline(labels):\n    return torch.FloatTensor(labels)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T12:20:33.917553Z","iopub.execute_input":"2025-03-30T12:20:33.917765Z","iopub.status.idle":"2025-03-30T12:20:33.934345Z","shell.execute_reply.started":"2025-03-30T12:20:33.917747Z","shell.execute_reply":"2025-03-30T12:20:33.933430Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"## 5. Dataset Preparation\n\n**Dataset Structure**:\n\n- Handles variable-length sequences\n\n- Implements length-based padding\n\n- Multi-label output (6 toxicity categories)","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom torch.nn.utils.rnn import pad_sequence\nimport torch\n\nclass PaddedDataset(Dataset):\n    def __init__(self, df, vocab, max_length=None):\n        self.df = df\n        self.vocab = vocab\n        self.max_length = max_length\n        self.label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        text = self.df.iloc[idx]['comment_text']\n        labels = self.df.iloc[idx][self.label_cols].values.astype(float)\n        \n        # Tokenize and numericalize\n        tokens = tokenizer(preprocess(text))\n        if self.max_length:\n            tokens = tokens[:self.max_length]\n        numericalized = [self.vocab[token] for token in tokens]\n        \n        return torch.tensor(numericalized, dtype=torch.long), torch.tensor(labels, dtype=torch.float)\n\n\ndef collate_batch(batch):\n    texts, labels = zip(*batch)\n    lengths = torch.tensor([len(t) for t in texts])\n     # Filter invalid sequences (length <=0)\n    valid_mask = lengths > 0\n    if not valid_mask.all():\n        texts = [t for t, valid in zip(texts, valid_mask) if valid]\n        labels = [l for l, valid in zip(labels, valid_mask) if valid]\n        lengths = lengths[valid_mask]\n    \n    # Add fallback for empty batch\n    if len(texts) == 0:\n        return torch.zeros((1,1), dtype=torch.long), torch.zeros((1,6)), torch.tensor([1])\n    # Pad sequences to match longest in batch\n    padded_texts = torch.nn.utils.rnn.pad_sequence(\n        texts, \n        batch_first=True, \n        padding_value=PAD_IDX\n    )\n    \n    return padded_texts, torch.stack(labels), lengths","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T12:20:33.935451Z","iopub.execute_input":"2025-03-30T12:20:33.935667Z","iopub.status.idle":"2025-03-30T12:20:33.949094Z","shell.execute_reply.started":"2025-03-30T12:20:33.935648Z","shell.execute_reply":"2025-03-30T12:20:33.948462Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"BATCH_SIZE = 64\nMAX_SEQ_LEN = 256\n\ndataset = PaddedDataset(training_data, vocab, max_length=MAX_SEQ_LEN)\ndataloader = DataLoader(\n    dataset, \n    batch_size=BATCH_SIZE, \n    shuffle=True, \n    collate_fn=collate_batch,\n    pin_memory=True,  # Faster data transfer to GPU\n    num_workers=2     # Parallel data loading\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T12:20:33.949861Z","iopub.execute_input":"2025-03-30T12:20:33.950103Z","iopub.status.idle":"2025-03-30T12:20:33.965320Z","shell.execute_reply.started":"2025-03-30T12:20:33.950073Z","shell.execute_reply":"2025-03-30T12:20:33.964485Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"## 6. Model Architectures","metadata":{}},{"cell_type":"markdown","source":"### 6.1 Baseline LSTM\n\nEmbedding → LSTM → Linear Layer","metadata":{}},{"cell_type":"code","source":"class lstm(nn.Module):\n    def __init__(self, vocab_size, embed_dim, hidden_dim, pad_idx, output_dim):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx = pad_idx)\n        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n        self.fc1 = nn.Linear(hidden_dim, output_dim)\n    def forward(self, text, lengths):\n        embedded = self.embedding(text)\n        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n            embedded,\n            lengths.cpu(),\n            batch_first=True,\n            enforce_sorted = False\n        )\n        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n        out = self.fc1(hidden[-1])\n        return torch.sigmoid(out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T12:20:33.966172Z","iopub.execute_input":"2025-03-30T12:20:33.966468Z","iopub.status.idle":"2025-03-30T12:20:33.983167Z","shell.execute_reply.started":"2025-03-30T12:20:33.966439Z","shell.execute_reply":"2025-03-30T12:20:33.982397Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"### 6.2 Bidirectional LSTM\n\n**Improvements**:\n\n-  Bidirectional\n-  processingDropout\n-  regularization\n","metadata":{}},{"cell_type":"code","source":"class BIDirectional_lstm(nn.Module):\n    def __init__(self, vocab_size, embed_dim, hidden_dim, pad_idx, output_dim):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx = pad_idx)\n        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n        self.fc1 = nn.Linear(hidden_dim * 2, output_dim)\n        self.dropout =  nn.Dropout(p = 0.3)\n    def forward(self, text, lengths):\n        embedded = self.embedding(text)\n        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n            embedded,\n            lengths.cpu(),\n            batch_first=True,\n            enforce_sorted = False\n        )\n        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n        hidden_output = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n        out = self.fc1(hidden_output)\n        out = self.dropout(out)\n        return torch.sigmoid(out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T12:20:33.984096Z","iopub.execute_input":"2025-03-30T12:20:33.984373Z","iopub.status.idle":"2025-03-30T12:20:33.998071Z","shell.execute_reply.started":"2025-03-30T12:20:33.984346Z","shell.execute_reply":"2025-03-30T12:20:33.997316Z"}},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"## 7. Training Framework","metadata":{}},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, epochs, learning_rate, filename):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    criterion = nn.BCELoss()\n    best_val_loss = float('inf')\n    model = model.to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)\n    \n    for epoch in range(epochs):\n        # Training Phase\n        model.train()\n        train_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for texts, labels, lengths in tqdm(train_loader, desc=f'Epoch {epoch+1}'):\n            # Move data to device\n            texts, labels = texts.to(device), labels.to(device)\n            \n            # Zero gradients\n            optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = model(texts, lengths)\n            loss = criterion(outputs, labels)\n            \n            # Backward pass and optimize\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n            optimizer.step()\n            \n            # Calculate metrics\n            train_loss += loss.item()\n            predicted = (outputs > 0.5).float()\n            correct += (predicted == labels).all(dim=1).sum().item()\n            total += labels.size(0)\n        \n        # Validation Phase\n        model.eval()\n        val_loss = 0.0\n        val_correct = 0\n        val_total = 0\n        \n        with torch.no_grad():\n            for texts, labels, lengths in val_loader:\n                texts, labels = texts.to(device), labels.to(device)\n                outputs = model(texts, lengths)\n                loss = criterion(outputs, labels)\n                \n                val_loss += loss.item()\n                predicted = (outputs > 0.5).float()\n                val_correct += (predicted == labels).all(dim=1).sum().item()\n                val_total += labels.size(0)\n        \n        # Epoch Statistics\n        train_loss /= len(train_loader)\n        train_acc = correct / total\n        val_loss /= len(val_loader)\n        val_acc = val_correct / val_total\n        \n        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n        print(f\"Train Loss: {train_loss:.4f} | Acc: {train_acc:.2%}\")\n        print(f\"Val Loss: {val_loss:.4f} | Acc: {val_acc:.2%}\")\n        \n        # Learning rate scheduling\n        scheduler.step(val_loss)\n        \n        # Save best model\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), filename + '.pth')\n    \n    print(\"Training complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T12:20:34.000820Z","iopub.execute_input":"2025-03-30T12:20:34.001013Z","iopub.status.idle":"2025-03-30T12:20:34.016505Z","shell.execute_reply.started":"2025-03-30T12:20:34.000996Z","shell.execute_reply":"2025-03-30T12:20:34.015794Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"\ntrain_df, val_df = train_test_split(training_data.iloc[:, 1:], test_size=0.2)\n\ntrain_dataset = PaddedDataset(train_df, vocab, max_length=256)\nval_dataset = PaddedDataset(val_df, vocab, max_length=256)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=64,\n    shuffle=True,\n    collate_fn=collate_batch,\n    pin_memory=True,\n    num_workers=4\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=64,\n    collate_fn=collate_batch,\n    pin_memory=True,\n    num_workers=4\n)\n\n\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T12:20:34.018250Z","iopub.execute_input":"2025-03-30T12:20:34.018442Z","iopub.status.idle":"2025-03-30T12:20:34.075041Z","shell.execute_reply.started":"2025-03-30T12:20:34.018426Z","shell.execute_reply":"2025-03-30T12:20:34.074423Z"},"scrolled":true},"outputs":[],"execution_count":32},{"cell_type":"code","source":"lstm_model = lstm(\n    vocab_size=len(vocab),\n    embed_dim=50,\n    hidden_dim=256,\n    output_dim=6,\n    pad_idx=PAD_IDX\n)\n\ntrain_model(lstm_model, train_loader, val_loader, 5, 0.001, \"lstm\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T12:20:34.075813Z","iopub.execute_input":"2025-03-30T12:20:34.076114Z"}},"outputs":[{"name":"stderr","text":"Epoch 1: 100%|██████████| 1995/1995 [01:06<00:00, 29.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/5\nTrain Loss: 0.0972 | Acc: 90.21%\nVal Loss: 0.0635 | Acc: 91.21%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 1995/1995 [01:07<00:00, 29.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2/5\nTrain Loss: 0.0545 | Acc: 91.68%\nVal Loss: 0.0531 | Acc: 91.65%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 1995/1995 [01:07<00:00, 29.61it/s]\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"BIDirectional_model = BIDirectional_lstm(\n    vocab_size=len(vocab),\n    embed_dim=50,\n    hidden_dim=256,\n    output_dim=6,\n    pad_idx=PAD_IDX\n)\n\ntrain_model(BIDirectional_model, train_loader, val_loader, 7, 0.0005, \"bidirctional_lstm\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. additional models","metadata":{}},{"cell_type":"markdown","source":"### 8.1 using pretrained embedding","metadata":{}},{"cell_type":"code","source":"def load_glove_from_file(glove_file):\n    word_to_vec = {}\n    with open(glove_file, 'r', encoding='utf-8') as f:\n        for line in tqdm(f, desc=\"Loading GloVe\"):\n            parts = line.split()\n            word = parts[0]\n            vector = np.array([float(val) for val in parts[1:]], dtype=np.float32)\n            word_to_vec[word] = vector\n    return word_to_vec\n\n# 1. Load your GloVe file\nglove_path = r\"/kaggle/input/glove-embeddings/glove.6B.100d.txt\"  # Update with your path\nglove_vectors = load_glove_from_file(glove_path)\n\n# 2. Create embedding matrix aligned with your vocabulary\ndef create_embedding_matrix(vocab, embedding_dim=100):\n    vocab_size = len(vocab)\n    weights = torch.zeros(vocab_size, embedding_dim)\n    \n    for word, idx in vocab.get_stoi().items():\n        if word in glove_vectors:\n            weights[idx] = torch.tensor(glove_vectors[word])\n        elif word == \"<pad>\":\n            weights[idx] = torch.zeros(embedding_dim)  # Pad token\n        else:\n            # Initialize unknown words randomly\n            weights[idx] = torch.randn(embedding_dim) * 0.25\n            \n    return weights\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class BI_lstm_GloVe_model(nn.Module):\n    def __init__(self, vocab_size, embed_dim, hidden_dim, pad_idx, output_dim):\n        super().__init__()\n        # Initialize with GloVe weights\n        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n        \n        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Bidirectional\n\n    def forward(self, text, lengths):\n        embedded = self.embedding(text)\n        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n            embedded,\n            lengths.cpu(),\n            batch_first=True,\n            enforce_sorted = False\n        )\n        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n        hidden_output = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n        out = self.fc(hidden_output)\n        return torch.sigmoid(out)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embedding_dim = 100  # matches GloVe dimension\nvocab_size = len(vocab)\nbi_lstm_glove_model = BI_lstm_GloVe_model(\n    vocab_size,\n    embedding_dim, \n    hidden_dim = 256,\n    pad_idx = PAD_IDX,\n    output_dim = 6\n)\n\n# Create embedding matrix\nweights = create_embedding_matrix(vocab, 100)\n# Assign to model\nbi_lstm_glove_model.embedding = nn.Embedding.from_pretrained(weights, freeze=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_model(bi_lstm_glove_model, train_loader, val_loader, 10, 0.0005, \"bi_lstm_glove\")","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 8.2 Additional improvments\n**Improvments**:\n- stacked bi-lstm\n- added basic attention mechanism and normalization layers","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**THIS IS IMPORTANT**","metadata":{}},{"cell_type":"code","source":"class Improved_BI_LSTM_GloVe(nn.Module):\n    def __init__(self, vocab_size, embed_dim, hidden_dim, pad_idx, output_dim):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n        \n        # Enhanced Architecture\n        self.lstm = nn.LSTM(embed_dim, hidden_dim, \n                           num_layers=2,              # Stacked LSTMs\n                           bidirectional=True, \n                           batch_first=True,\n                           dropout=0.3)               # Inter-layer dropout\n        \n        self.attention = nn.Linear(hidden_dim * 2, 1) # Simple attention mechanism\n        self.bn1 = nn.BatchNorm1d(hidden_dim * 2)     # Batch normalization\n        \n        self.fc = nn.Sequential(\n            nn.Linear(hidden_dim * 2, hidden_dim),\n            nn.ReLU(),\n            nn.BatchNorm1d(hidden_dim),\n            nn.Dropout(0.5),                          # Increased dropout\n            nn.Linear(hidden_dim, output_dim)\n        )\n        \n        # Initialize with kaiming normal for better convergence\n        for layer in [self.attention, *self.fc]:\n            if isinstance(layer, nn.Linear):\n                nn.init.kaiming_normal_(layer.weight)\n\n    def forward(self, text, lengths):\n        # Embedding with dropout\n        embedded = F.dropout(self.embedding(text), p=0.2, training=self.training)\n        \n        # Packed sequence\n        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n            embedded, lengths.cpu(), batch_first=True, enforce_sorted=False\n        )\n        \n        # BiLSTM with 2 layers\n        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n        \n        # Attention mechanism\n        attention_weights = F.softmax(self.attention(output), dim=1)\n        context_vector = torch.sum(attention_weights * output, dim=1)\n        \n        # Batch norm + FC\n        context_vector = self.bn1(context_vector)\n        return self.fc(context_vector)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, epochs, learning_rate, filename):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    criterion = nn.BCEWithLogitsLoss()\n    best_val_loss = float('inf')\n    model = model.to(device)\n    optimizer = torch.optim.AdamW(model.parameters())\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n    for epoch in range(epochs):\n        # Training Phase\n        model.train()\n        train_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for texts, labels, lengths in tqdm(train_loader, desc=f'Epoch {epoch+1}'):\n            # Move data to device\n            texts, labels = texts.to(device), labels.to(device)\n            \n            # Zero gradients\n            optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = model(texts, lengths)\n            loss = criterion(outputs, labels)\n            \n            # Backward pass and optimize\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n            optimizer.step()\n            \n            # Calculate metrics\n            train_loss += loss.item()\n            predicted = (outputs > 0.5).float()\n            correct += (predicted == labels).all(dim=1).sum().item()\n            total += labels.size(0)\n        \n        # Validation Phase\n        model.eval()\n        val_loss = 0.0\n        val_correct = 0\n        val_total = 0\n        \n        with torch.no_grad():\n            for texts, labels, lengths in val_loader:\n                texts, labels = texts.to(device), labels.to(device)\n                outputs = model(texts, lengths)\n                loss = criterion(outputs, labels)\n                \n                val_loss += loss.item()\n                predicted = (outputs > 0.5).float()\n                val_correct += (predicted == labels).all(dim=1).sum().item()\n                val_total += labels.size(0)\n        \n        # Epoch Statistics\n        train_loss /= len(train_loader)\n        train_acc = correct / total\n        val_loss /= len(val_loader)\n        val_acc = val_correct / val_total\n        \n        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n        print(f\"Train Loss: {train_loss:.4f} | Acc: {train_acc:.2%}\")\n        print(f\"Val Loss: {val_loss:.4f} | Acc: {val_acc:.2%}\")\n        \n        # Learning rate scheduling\n        scheduler.step(val_loss)\n        \n        # Save best model\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), filename + '.pth')\n    \n    print(\"Training complete!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_model = Improved_BI_LSTM_GloVe(\n    vocab_size=len(vocab),\n    embed_dim=100,\n    hidden_dim=256,\n    pad_idx=PAD_IDX,\n    output_dim=6\n)\nfinal_model.embedding.weight.data.copy_(weights)\nfinal_model.embedding.weight.requires_grad = True\ntrain_model(final_model, train_loader, val_loader, 10, 0.0001, \"final\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9. Model Evaluation\n\n**Evaluation Metrics**:\n\n- Average class-wise ROC-AUC scores as used in the competition\n\n- Batch-wise processing for memory efficiency","metadata":{}},{"cell_type":"code","source":"ev_data = pd.concat([test_data,test_labels_data.iloc[:,1:]], axis=1)\n# dropping -1 rows, these rows weren't used for evaluation models in the competetion and marked with -1 \nev_data = ev_data[ev_data['toxic']!= -1]\nev_data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"eval_dataset = PaddedDataset(ev_data, vocab, max_length=256)\neval_loader = DataLoader(\n    eval_dataset,\n    batch_size=512,\n    collate_fn=collate_batch,\n    pin_memory=True,\n    num_workers=4\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calc_roc(model):\n    model.eval()\n    all_labels = []\n    all_outputs = []\n    \n    with torch.no_grad():\n        for texts, labels, lengths in eval_loader:\n            texts, labels = texts.to(device), labels.to(device)\n            outputs = model(texts, lengths)\n            \n            # Store batch results\n            all_labels.append(labels.cpu().numpy())\n            all_outputs.append(outputs.cpu().numpy())\n    \n    # Concatenate all batches\n    all_labels = np.concatenate(all_labels, axis=0)\n    all_outputs = np.concatenate(all_outputs, axis=0)\n    \n    # Calculate ROC-AUC for each class\n    roc_scores = []\n    for col in range(6):  # the original evaluation method is to take ROC-AUC scores average for the 6 classed\n        if np.sum(all_labels[:, col]) > 0:\n            roc = roc_auc_score(all_labels[:, col], all_outputs[:, col])\n            roc_scores.append(roc)\n    \n    # Return average\n    return np.mean(roc_scores)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 10. Results\n","metadata":{}},{"cell_type":"code","source":"lstm_model = lstm_model = lstm(\n    vocab_size=len(vocab),\n    embed_dim=50,\n    hidden_dim=256,\n    output_dim=6,\n    pad_idx=PAD_IDX\n).to(device)\n\nbi_lstm_model = BIDirectional_model = BIDirectional_lstm(\n    vocab_size=len(vocab),\n    embed_dim=50,\n    hidden_dim=256,\n    output_dim=6,\n    pad_idx=PAD_IDX\n).to(device)\n\nvocab_size = len(vocab)\nbi_lstm_glove_model = BI_lstm_GloVe_model(\n    vocab_size,\n    100, \n    hidden_dim = 256,\n    pad_idx = PAD_IDX,\n    output_dim = 6\n).to(device)\n\nfinal_model = Improved_BI_LSTM_GloVe(\n    vocab_size=len(vocab),\n    embed_dim=100,\n    hidden_dim=256,\n    pad_idx=PAD_IDX,\n    output_dim=6\n).to(device)\n\nlstm_model.load_state_dict(torch.load(\"lstm.pth\"))\nbi_lstm_model.load_state_dict(torch.load(\"bidirctional_lstm.pth\"))\nbi_lstm_glove_model.load_state_dict(torch.load(\"bi_lstm_glove.pth\"))\nfinal_model.load_state_dict(torch.load(\"final.pth\"))\n\nprint(\"lstm model roc-auc \", calc_roc(lstm_model))\nprint(\"BiDirectional lstm model roc-auc \", calc_roc(bi_lstm_model))\nprint(\"BiDirectional lstm with pretrained embedding model roc-auc \", calc_roc(bi_lstm_glove_model))\nprint(\"stacked Bidirectional lstm model with pretrained embedding roc-auc \", calc_roc(final_model))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lstm_model = torch.load(\"/kaggle/working/lstm.pth\")\n# bi_lstm_model.load_state_dict(torch.load(\"bidirctional_lstm.pth\"))\n# bi_lstm_glove_model.load_state_dict(torch.load(\"bi_lstm_glove.pth\"))\nfinal_model = torch.load(\"/kaggle/working/final.pth\") \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T12:19:07.886937Z","iopub.status.idle":"2025-03-30T12:19:07.887307Z","shell.execute_reply":"2025-03-30T12:19:07.887152Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/rahul2/dataset_instagram-scraper_2025-03-30_09-30-50-381.csv\")\ntest_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TestDataset(Dataset):\n    def __init__(self, df, vocab, max_length=None):\n        self.df = df.reset_index(drop=True)  # Ensure clean indices\n        self.vocab = vocab\n        self.max_length = max_length\n        self.valid_indices = []  # Will track which indices were valid\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        text = str(self.df.iloc[idx]['text'])\n        tokens = tokenizer(preprocess(text))\n        \n        # Track valid (non-empty) samples\n        if len(tokens) > 0:\n            if idx not in self.valid_indices:\n                self.valid_indices.append(idx)\n        else:\n            return torch.tensor([], dtype=torch.long)  # Return empty for invalid\n            \n        if self.max_length:\n            tokens = tokens[:self.max_length]\n        numericalized = [self.vocab[token] for token in tokens]\n        return torch.tensor(numericalized, dtype=torch.long)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collate_test_batch(batch):\n    # Track original indices of valid samples\n    valid_indices = [i for i, t in enumerate(batch) if len(t) > 0]\n    texts = [t for t in batch if len(t) > 0]\n    \n    if not texts:\n        # Return dummy batch with empty indices\n        return torch.zeros((1, 1)), torch.tensor([1]), []\n        \n    lengths = torch.tensor([len(t) for t in texts])\n    padded = pad_sequence(texts, batch_first=True, padding_value=PAD_IDX)\n    return padded, lengths, valid_indices  # Now returns indices","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset = TestDataset(test_df, vocab, max_length=MAX_SEQ_LEN)\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,  # Critical for maintaining order!\n    collate_fn=collate_test_batch,\n    pin_memory=True,\n    num_workers=2\n)# Create dataset and loader\ntest_dataset = TestDataset(test_df, vocab, max_length=MAX_SEQ_LEN)\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    collate_fn=collate_test_batch,\n    num_workers=2\n)\n\n# Get predictions\npreds, probs = predict_with_loader(lstm_model, test_loader, device, len(test_df))\n\n# Add to DataFrame\ntest_df['predicted_class'] = preds\nfor i, name in enumerate(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']):\n    test_df[f'prob_{name}'] = probs[:, i]\n\n# Mark predicted rows (only if you need this)\ntest_df['was_predicted'] = False\ntest_df.loc[test_dataset.valid_indices, 'was_predicted'] = True  # Note: test_dataset, not loader","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(test_df.columns.tolist())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"eval_dataset2 = PaddedDataset(ev_data, vocab, max_length=256)\neval_loader = DataLoader(\n    eval_dataset,\n    batch_size=512,\n    collate_fn=collate_batch,\n    pin_memory=True,\n    num_workers=4\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/rahul2/dataset_instagram-scraper_2025-03-30_09-30-50-381.csv\")\ntest_df.rename(columns={'text': 'comment_text'}, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T12:09:39.324103Z","iopub.execute_input":"2025-03-30T12:09:39.324408Z","iopub.status.idle":"2025-03-30T12:09:39.362443Z","shell.execute_reply.started":"2025-03-30T12:09:39.324387Z","shell.execute_reply":"2025-03-30T12:09:39.361556Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"vocab = torch.load(\"/kaggle/working/vocab.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T12:16:19.512534Z","iopub.execute_input":"2025-03-30T12:16:19.512852Z","iopub.status.idle":"2025-03-30T12:16:19.533320Z","shell.execute_reply.started":"2025-03-30T12:16:19.512826Z","shell.execute_reply":"2025-03-30T12:16:19.532128Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-6d17fe81a8e1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/working/vocab.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 998\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    999\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/vocab.pth'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/working/vocab.pth'","output_type":"error"}],"execution_count":17},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport re\nimport string\nimport emoji\nfrom nltk.corpus import stopwords\nimport spacy\nfrom torchtext.data.utils import get_tokenizer\nfrom torch.nn.utils.rnn import pad_sequence\nimport numpy as np\n\nclass ToxicityClassifierPipeline:\n    def __init__(self, model, vocab_path=\"vocab.pth\"):\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model = model.to(self.device)\n        self.model.eval()\n        self.tokenizer = get_tokenizer(\"basic_english\")\n        self.vocab = torch.load(vocab_path)\n        self.PAD_IDX = self.vocab['<pad>']\n        self.nlp = spacy.load(\"en_core_web_sm\")\n        self.stpwds = stopwords.words('english')\n        \n        # Define preprocessing patterns\n        self.punc = string.punctuation.replace('#', '').replace('!', '').replace('?', '') + \"∞θ÷α•à−β∅³π‘₹´°£€\\\\×™√²—\"\n        self.patterns = [\n            r'\\\\[nrtbfv\\\\]',         # \\n, \\t etc\n            '<.*?>',                 # HTML tags\n            r'https?://\\S+|www\\.\\S+', # Links\n            r'\\ufeff',               # BOM characters\n            r'^[^a-zA-Z0-9]+$',      # Non-alphanumeric tokens\n            r'ｗｗｗ．\\S+',            # Full-width URLs\n            r'[\\uf700-\\uf7ff]',      # Unicode private-use chars\n            r'^[－—…]+$',            # Special punctuation\n            r'[︵︶]'                # CJK parentheses\n        ]\n        \n        # Chat words mapping (truncated for brevity)\n        self.chat_words = {\n            \"AFAIK\": \"As Far As I Know\",\n            \"AFK\": \"Away From Keyboard\",\n            # ... include all your chat words mapping\n        }\n        \n        self.time_zone_abbreviations = [\n            \"UTC\", \"GMT\", \"EST\", \"CST\", \"PST\", \"MST\",\n            \"EDT\", \"CDT\", \"PDT\", \"MDT\", \"CET\", \"EET\",\n            \"WET\", \"AEST\", \"ACST\", \"AWST\", \"HST\",\n            \"AKST\", \"IST\", \"JST\", \"KST\", \"NZST\"\n        ]\n\n    def preprocess_text(self, text):\n        \"\"\"Apply all preprocessing steps to a single text\"\"\"\n        if not isinstance(text, str) or not text.strip():\n            return \"\"\n            \n        # Apply regex patterns\n        for regex in self.patterns:\n            text = re.sub(regex, '', text)\n            \n        # Remove punctuation\n        text = text.translate(str.maketrans(self.punc, ' ' * len(self.punc)))\n        \n        # Remove time zones and stopwords\n        text = ' '.join(word for word in text.split() \n                       if word not in self.time_zone_abbreviations \n                       and word not in self.stpwds)\n        \n        # Expand chat words\n        text = ' '.join(self.chat_words.get(word.lower(), word) for word in text.split())\n        \n        # Lowercase and emoji handling\n        text = text.lower()\n        text = emoji.demojize(text)\n        text = re.sub(r'\\s+', ' ', text).strip()\n        \n        return text\n\n    def tokenize_and_numericalize(self, text, max_length=256):\n        \"\"\"Tokenize and convert to numerical tokens\"\"\"\n        if not text:  # Handle empty text\n            return torch.empty(0, dtype=torch.long)\n            \n        tokens = [token for token in self.tokenizer(text) if 1 < len(token) < 25]\n        tokens = tokens[:max_length]\n        numericalized = [self.vocab[token] if token in self.vocab else self.vocab['<unk>'] \n                        for token in tokens]\n        return torch.tensor(numericalized, dtype=torch.long)\n\n    def predict_toxicity(self, df, text_column='comment_text', batch_size=64):\n        \"\"\"\n        Predict toxicity for a DataFrame of texts\n        \n        Args:\n            df: Input DataFrame containing text to classify\n            text_column: Name of column containing text\n            batch_size: Batch size for prediction\n            \n        Returns:\n            DataFrame with original text and toxicity predictions\n        \"\"\"\n        # Create a copy of the original DataFrame to preserve indices\n        result_df = df.copy()\n        \n        # Preprocess all texts and keep track of non-empty texts\n        processed_data = []\n        valid_indices = []\n        \n        for idx, text in enumerate(df[text_column]):\n            processed = self.preprocess_text(text)\n            if processed:  # Only keep non-empty texts\n                processed_data.append(processed)\n                valid_indices.append(idx)\n        \n        # If all texts are empty after preprocessing\n        if not processed_data:\n            # Return all zeros for all predictions\n            result_df['toxic'] = 0\n            result_df['severe_toxic'] = 0\n            result_df['obscene'] = 0\n            result_df['threat'] = 0\n            result_df['insult'] = 0\n            result_df['identity_hate'] = 0\n            return result_df\n        \n        # Tokenize and numericalize only non-empty texts\n        tokenized = [self.tokenize_and_numericalize(text) for text in processed_data]\n        \n        # Create batches only for valid sequences\n        batches = []\n        for i in range(0, len(tokenized), batch_size):\n            batch_texts = tokenized[i:i+batch_size]\n            lengths = torch.tensor([len(t) for t in batch_texts])\n            \n            # Filter out empty sequences in this batch\n            valid_mask = lengths > 0\n            if not valid_mask.any():\n                continue\n                \n            batch_texts = [t for t, valid in zip(batch_texts, valid_mask) if valid]\n            lengths = lengths[valid_mask]\n            \n            # Pad sequences\n            padded = pad_sequence(batch_texts, batch_first=True, padding_value=self.PAD_IDX)\n            batches.append((padded, lengths, valid_mask))\n        \n        # Make predictions\n        all_preds = np.zeros((len(df), 6), dtype=int)  # Initialize with zeros\n        \n        with torch.no_grad():\n            current_idx = 0\n            for batch, lengths, valid_mask in batches:\n                batch = batch.to(self.device)\n                outputs = self.model(batch, lengths.to(self.device))\n                preds = (outputs > 0.5).int().cpu().numpy()\n                \n                # Assign predictions to the correct positions\n                batch_size = len(preds)\n                for i in range(batch_size):\n                    if current_idx + i < len(valid_indices):\n                        all_preds[valid_indices[current_idx + i]] = preds[i]\n                \n                current_idx += batch_size\n        \n        # Add predictions to result DataFrame\n        result_df['toxic'] = all_preds[:, 0]\n        result_df['severe_toxic'] = all_preds[:, 1]\n        result_df['obscene'] = all_preds[:, 2]\n        result_df['threat'] = all_preds[:, 3]\n        result_df['insult'] = all_preds[:, 4]\n        result_df['identity_hate'] = all_preds[:, 5]\n        \n        return result_df\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Load your trained model (example using the final model)\n    final_model = Improved_BI_LSTM_GloVe(\n        vocab_size=len(vocab),\n        embed_dim=100,\n        hidden_dim=256,\n        pad_idx=PAD_IDX,\n        output_dim=6\n    )\n    final_model.load_state_dict(torch.load(\"final.pth\"))\n    \n    # Initialize pipeline\n    pipeline = ToxicityClassifierPipeline(final_model)\n    \n    # Example test DataFrame\n    test_df = pd.read_csv(\"/kaggle/input/rahul2/dataset_instagram-scraper_2025-03-30_09-30-50-381.csv\")\n    test_df.rename(columns={'text': 'comment_text'}, inplace=True)\n    \n    # Get predictions\n    results = pipeline.predict_toxicity(test_df)\n    print(results[['comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T12:10:56.829736Z","iopub.execute_input":"2025-03-30T12:10:56.830103Z","iopub.status.idle":"2025-03-30T12:10:56.861795Z","shell.execute_reply.started":"2025-03-30T12:10:56.830074Z","shell.execute_reply":"2025-03-30T12:10:56.860758Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-93a768c117ab>\u001b[0m in \u001b[0;36m<cell line: 175>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;31m# Load your trained model (example using the final model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     final_model = Improved_BI_LSTM_GloVe(\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0membed_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'vocab' is not defined"],"ename":"NameError","evalue":"name 'vocab' is not defined","output_type":"error"}],"execution_count":14},{"cell_type":"code","source":"results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T11:49:45.680042Z","iopub.execute_input":"2025-03-30T11:49:45.680371Z","iopub.status.idle":"2025-03-30T11:49:45.690164Z","shell.execute_reply.started":"2025-03-30T11:49:45.680338Z","shell.execute_reply":"2025-03-30T11:49:45.689295Z"}},"outputs":[{"execution_count":115,"output_type":"execute_result","data":{"text/plain":"                   id            comment_text  toxic  severe_toxic  obscene  \\\n0   18069096703645733   Why bro in sea always      0             0        0   \n1   18052302512112156    😂😂😂😂😂😂😂 misericórdia      0             0        0   \n2   18119491555435268                       🙌      0             0        0   \n3   17884548831142635                       🔥      0             0        0   \n4   18059375443875265              nigga what      1             1        1   \n5   18097418647542771  Vessel of NBA youngboy      0             0        0   \n6   18071614537687683                JAJAJAJA      0             0        0   \n7   17947968803927907                     wtf      1             1        1   \n8   17916781080051191         fuck that bitch      1             1        1   \n9   18036427937403276            Is this real      0             0        0   \n10  17865902130262070           kill yourself      1             1        1   \n11  18045930944093198         @hyunmin._.x 고고      0             0        0   \n12  18050583443465416              @zellys_1w      0             0        0   \n13  17998000631608824           @eluruchinche      0             0        0   \n14  17915460810066302         La @ha__neth xd      0             0        0   \n\n    threat  insult  identity_hate  \n0        0       0              0  \n1        0       0              0  \n2        0       0              0  \n3        0       0              0  \n4        0       1              1  \n5        0       0              0  \n6        0       0              0  \n7        0       1              0  \n8        0       1              1  \n9        0       0              0  \n10       1       1              1  \n11       0       0              0  \n12       0       0              0  \n13       0       0              0  \n14       0       0              0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>18069096703645733</td>\n      <td>Why bro in sea always</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>18052302512112156</td>\n      <td>😂😂😂😂😂😂😂 misericórdia</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>18119491555435268</td>\n      <td>🙌</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>17884548831142635</td>\n      <td>🔥</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>18059375443875265</td>\n      <td>nigga what</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>18097418647542771</td>\n      <td>Vessel of NBA youngboy</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>18071614537687683</td>\n      <td>JAJAJAJA</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>17947968803927907</td>\n      <td>wtf</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>17916781080051191</td>\n      <td>fuck that bitch</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>18036427937403276</td>\n      <td>Is this real</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>17865902130262070</td>\n      <td>kill yourself</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>18045930944093198</td>\n      <td>@hyunmin._.x 고고</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>18050583443465416</td>\n      <td>@zellys_1w</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>17998000631608824</td>\n      <td>@eluruchinche</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>17915460810066302</td>\n      <td>La @ha__neth xd</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":115},{"cell_type":"code","source":"final_ans = results.drop('identity_hate',axis = 1)\nfinal_ans","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T11:49:47.555398Z","iopub.execute_input":"2025-03-30T11:49:47.555737Z","iopub.status.idle":"2025-03-30T11:49:47.565764Z","shell.execute_reply.started":"2025-03-30T11:49:47.555713Z","shell.execute_reply":"2025-03-30T11:49:47.564894Z"}},"outputs":[{"execution_count":116,"output_type":"execute_result","data":{"text/plain":"                   id            comment_text  toxic  severe_toxic  obscene  \\\n0   18069096703645733   Why bro in sea always      0             0        0   \n1   18052302512112156    😂😂😂😂😂😂😂 misericórdia      0             0        0   \n2   18119491555435268                       🙌      0             0        0   \n3   17884548831142635                       🔥      0             0        0   \n4   18059375443875265              nigga what      1             1        1   \n5   18097418647542771  Vessel of NBA youngboy      0             0        0   \n6   18071614537687683                JAJAJAJA      0             0        0   \n7   17947968803927907                     wtf      1             1        1   \n8   17916781080051191         fuck that bitch      1             1        1   \n9   18036427937403276            Is this real      0             0        0   \n10  17865902130262070           kill yourself      1             1        1   \n11  18045930944093198         @hyunmin._.x 고고      0             0        0   \n12  18050583443465416              @zellys_1w      0             0        0   \n13  17998000631608824           @eluruchinche      0             0        0   \n14  17915460810066302         La @ha__neth xd      0             0        0   \n\n    threat  insult  \n0        0       0  \n1        0       0  \n2        0       0  \n3        0       0  \n4        0       1  \n5        0       0  \n6        0       0  \n7        0       1  \n8        0       1  \n9        0       0  \n10       1       1  \n11       0       0  \n12       0       0  \n13       0       0  \n14       0       0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>18069096703645733</td>\n      <td>Why bro in sea always</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>18052302512112156</td>\n      <td>😂😂😂😂😂😂😂 misericórdia</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>18119491555435268</td>\n      <td>🙌</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>17884548831142635</td>\n      <td>🔥</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>18059375443875265</td>\n      <td>nigga what</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>18097418647542771</td>\n      <td>Vessel of NBA youngboy</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>18071614537687683</td>\n      <td>JAJAJAJA</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>17947968803927907</td>\n      <td>wtf</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>17916781080051191</td>\n      <td>fuck that bitch</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>18036427937403276</td>\n      <td>Is this real</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>17865902130262070</td>\n      <td>kill yourself</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>18045930944093198</td>\n      <td>@hyunmin._.x 고고</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>18050583443465416</td>\n      <td>@zellys_1w</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>17998000631608824</td>\n      <td>@eluruchinche</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>17915460810066302</td>\n      <td>La @ha__neth xd</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":116},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}